{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled30.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r2Tlrnh0K-NN"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import mean\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql import functions as F\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.mllib.util import MLUtils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "K5zGJqW7P1zT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "uyai_hHjP4-H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv('weatherAUS.csv',inferSchema =True, header = True)\n",
        "dataset.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVHhvFDMP7pK",
        "outputId": "98b7c883-daa0-4a79-88d8-f9587613f53c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
            "|      Date|Location|MinTemp|MaxTemp|Rainfall|Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RainTomorrow|\n",
            "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
            "|2008-12-01|  Albury|   13.4|   22.9|     0.6|         NA|      NA|          W|           44|         W|       WNW|          20|          24|         71|         22|     1007.7|     1007.1|       8|      NA|   16.9|   21.8|       No|          No|\n",
            "|2008-12-02|  Albury|    7.4|   25.1|       0|         NA|      NA|        WNW|           44|       NNW|       WSW|           4|          22|         44|         25|     1010.6|     1007.8|      NA|      NA|   17.2|   24.3|       No|          No|\n",
            "|2008-12-03|  Albury|   12.9|   25.7|       0|         NA|      NA|        WSW|           46|         W|       WSW|          19|          26|         38|         30|     1007.6|     1008.7|      NA|       2|     21|   23.2|       No|          No|\n",
            "|2008-12-04|  Albury|    9.2|     28|       0|         NA|      NA|         NE|           24|        SE|         E|          11|           9|         45|         16|     1017.6|     1012.8|      NA|      NA|   18.1|   26.5|       No|          No|\n",
            "|2008-12-05|  Albury|   17.5|   32.3|       1|         NA|      NA|          W|           41|       ENE|        NW|           7|          20|         82|         33|     1010.8|       1006|       7|       8|   17.8|   29.7|       No|          No|\n",
            "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset  = dataset.drop(\"Evaporation\", \"Sunshine\", 'Cloud9am', 'Cloud3pm')\n",
        "dataset.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1xzwRqVQBtj",
        "outputId": "f67137e4-5a0c-40fd-be2c-6fb7af2a6107"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date',\n",
              " 'Location',\n",
              " 'MinTemp',\n",
              " 'MaxTemp',\n",
              " 'Rainfall',\n",
              " 'WindGustDir',\n",
              " 'WindGustSpeed',\n",
              " 'WindDir9am',\n",
              " 'WindDir3pm',\n",
              " 'WindSpeed9am',\n",
              " 'WindSpeed3pm',\n",
              " 'Humidity9am',\n",
              " 'Humidity3pm',\n",
              " 'Pressure9am',\n",
              " 'Pressure3pm',\n",
              " 'Temp9am',\n",
              " 'Temp3pm',\n",
              " 'RainToday',\n",
              " 'RainTomorrow']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2 = dataset.filter(dataset.RainTomorrow != \"NA\" )  \n",
        "(dataset2\n",
        ".select(\"RainTomorrow\")\n",
        ".groupBy(\"RainTomorrow\")\n",
        ".count()\n",
        ".orderBy(\"count\", ascending=False)\n",
        ".show(n=90, truncate=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uEMiIfMQQYh",
        "outputId": "5d53d9c0-7b9d-4c9f-d373-3edb58d427b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------+\n",
            "|RainTomorrow|count |\n",
            "+------------+------+\n",
            "|No          |110316|\n",
            "|Yes         |31877 |\n",
            "+------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "def mean_of_pyspark_columns(df, numeric_cols, verbose=False):\n",
        "    col_with_mean=[]\n",
        "    for col in numeric_cols:\n",
        "        mean_value = df.select(avg(df[col]))\n",
        "        avg_col = mean_value.columns[0]\n",
        "        res = mean_value.rdd.map(lambda row : row[avg_col]).collect()\n",
        "        \n",
        "        if (verbose==True): print(mean_value.columns[0], \"\\t\", res[0])\n",
        "        col_with_mean.append([col, res[0]])    \n",
        "    return col_with_mean"
      ],
      "metadata": {
        "id": "fmcnx8awQfGK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, lit\n",
        "\n",
        "def fill_missing_with_mean(df, numeric_cols):\n",
        "    col_with_mean = mean_of_pyspark_columns(df, numeric_cols) \n",
        "    \n",
        "    for col, mean in col_with_mean:\n",
        "        df = df.withColumn(col, when(df[col].isNull()==True, \n",
        "        lit(mean)).otherwise(df[col]))\n",
        "        \n",
        "    return df"
      ],
      "metadata": {
        "id": "hYhEG3a_vpJ8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = ['MinTemp','MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm']\n",
        "dataset2 = fill_missing_with_mean(dataset2, numeric_cols)\n",
        "cols = dataset.columns\n"
      ],
      "metadata": {
        "id": "Gw8Z6dQkQpPz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "\n",
        "from distutils.version import LooseVersion\n",
        "\n",
        "categoricalColumns = ['Date',  'Location',  'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']\n",
        "stages = [] # stages in Pipeline\n",
        "for categoricalCol in categoricalColumns:\n",
        "    # Category Indexing with StringIndexer\n",
        "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
        "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
        "    if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n",
        "        from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
        "    else:\n",
        "        from pyspark.ml.feature import OneHotEncoder\n",
        "        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
        "    # Add stages.  These are not run here, but will run all at once later on.\n",
        "    stages += [stringIndexer, encoder]"
      ],
      "metadata": {
        "id": "iNqHcL3cv1hz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert label into label indices using the StringIndexer\n",
        "label_stringIdx = StringIndexer(inputCol=\"RainTomorrow\", outputCol=\"label\")\n",
        "stages += [label_stringIdx]"
      ],
      "metadata": {
        "id": "XbqweCB8v4UY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numericCols = ['MinTemp','MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm']\n",
        "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
        "stages += [assembler]"
      ],
      "metadata": {
        "id": "10vp4Filv7i7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "partialPipeline = Pipeline().setStages(stages)\n",
        "pipelineModel = partialPipeline.fit(dataset2)\n",
        "preppedDataDF = pipelineModel.transform(dataset2)"
      ],
      "metadata": {
        "id": "e8NCFCnEwEkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model to prepped data\n",
        "lrModel = LogisticRegression().fit(preppedDataDF)\n",
        "\n",
        "# ROC for training data \n",
        "display(lrModel, preppedDataDF, \"ROC\")"
      ],
      "metadata": {
        "id": "YLs6qdkawJzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep relevant columns\n",
        "selectedcols = [\"label\", \"features\"] + cols\n",
        "dataset2 = preppedDataDF.select(selectedcols)\n",
        "dataset2.show(5)"
      ],
      "metadata": {
        "id": "Gft80Lx3x15i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(trainingData, testData) = dataset2.randomSplit([0.8, 0.2], seed=12345)\n",
        "print(trainingData.count())\n",
        "print(testData.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqIdmdh0x6KE",
        "outputId": "838e708a-011e-4d00-f162-1d55b17ba1fe"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "113672\n",
            "28521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "# Create initial Decision Tree Model\n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n",
        "\n",
        "# Train model with Training Data\n",
        "dtModel = dt.fit(trainingData)\n"
      ],
      "metadata": {
        "id": "Nyp3oprQyAbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test data using the Transformer.transform() method.\n",
        "predictions = dtModel.transform(testData)\n",
        "predictions.printSchema()"
      ],
      "metadata": {
        "id": "I-UiQ8zJyaJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View model's predictions and probabilities of each prediction class\n",
        "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"Location\", \"RainToday\")\n",
        "selected.show(5)"
      ],
      "metadata": {
        "id": "a-qv31Fmyg2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ParamGrid for Cross Validation\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(dt.maxBins, [5, 10, 15])\n",
        "             .addGrid(dt.minInfoGain, [0.0, 0.2, 0.4])\n",
        "             .addGrid(dt.maxDepth, [3, 5, 7])\n",
        "             .build())"
      ],
      "metadata": {
        "id": "l1SwXn_BykB-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt.getImpurity()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iq0N05Q2ymrl",
        "outputId": "4e18925e-d09f-4d66-a31f-a149e2bc7245"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gini'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_predictions = dtModel.transform(testData)"
      ],
      "metadata": {
        "id": "v0QZRQyPyxw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "multi_evaluator_ac = MulticlassClassificationEvaluator(labelCol = 'label', metricName = 'accuracy')\n",
        "multi_evaluator_f1 = MulticlassClassificationEvaluator(labelCol = 'label', metricName = 'f1')"
      ],
      "metadata": {
        "id": "CF5RKlq3yyxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Decision Tree Accuracy (gini):', multi_evaluator_ac.evaluate(dt_predictions))\n",
        "print('Decision Tree F1 (gini):', multi_evaluator_f1.evaluate(dt_predictions))"
      ],
      "metadata": {
        "id": "WTTFrfGKy0zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "# Evaluate model\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "id": "tiA6S7S4y3Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 3-fold CrossValidator\n",
        "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
        "\n",
        "# Run cross validations\n",
        "cvModel = cv.fit(trainingData)\n",
        "# Takes ~5 minutes"
      ],
      "metadata": {
        "id": "D_8fTg5Fy5aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use test set to measure the accuracy of the model on new data\n",
        "predictions = cvModel.transform(testData)"
      ],
      "metadata": {
        "id": "74dVJkXuy9HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cvModel uses the best model found from the Cross Validation\n",
        "# Evaluate best model\n",
        "evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "id": "7MSO-2Pxy_Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Best model's predictions and probabilities of each prediction class\n",
        "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"Location\", \"RainToday\")\n",
        "selected.show(5)"
      ],
      "metadata": {
        "id": "y6EkrQtlzEL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create initial Decision Tree Model\n",
        "dt1 = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", impurity= 'entropy', maxDepth=3)\n",
        "\n",
        "# Train model with Training Data\n",
        "dtModel1 = dt1.fit(trainingData)"
      ],
      "metadata": {
        "id": "jEPKIpnwzGUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test data using the Transformer.transform() method.\n",
        "predictions1 = dtModel1.transform(testData)"
      ],
      "metadata": {
        "id": "qTWjbCkNzILg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt1.getImpurity()"
      ],
      "metadata": {
        "id": "BpFoxOxlzKqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View model's predictions and probabilities of each prediction class\n",
        "selected1 = predictions1.select(\"label\", \"prediction\", \"probability\", \"Location\", \"RainToday\")\n",
        "selected1.show(5)"
      ],
      "metadata": {
        "id": "vcKVumOzzPTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Decisioin Tree with Binary Classification\n",
        "# Evaluate model\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "id": "1YcZ9e8wzQZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ParamGrid for Cross Validation\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "paramGrid1 = (ParamGridBuilder()\n",
        "             .addGrid(dt1.impurity, \"entropy\")\n",
        "             .addGrid(dt1.maxBins, [5, 10, 15])\n",
        "             .addGrid(dt1.minInfoGain, [0.0, 0.2, 0.4])\n",
        "             .addGrid(dt1.maxDepth, [3, 5, 7])\n",
        "             .build())"
      ],
      "metadata": {
        "id": "LakVx2EtzTNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 3-fold CrossValidator\n",
        "cv1 = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid1, evaluator=evaluator, numFolds=3)\n",
        "\n",
        "# Run cross validations\n",
        "cvModel1 = cv1.fit(trainingData)"
      ],
      "metadata": {
        "id": "wy_CspslzVzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use test set to measure the accuracy of the model on new data\n",
        "predictions1 = cvModel1.transform(testData)\n",
        "# cvModel uses the best model found from the Cross Validation\n",
        "# Evaluate best model\n",
        "evaluator.evaluate(predictions1)"
      ],
      "metadata": {
        "id": "l0lzQR3pzXmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Best model's predictions and probabilities of each prediction class\n",
        "selected1 = predictions1.select(\"label\", \"prediction\", \"probability\", \"Location\", \"RainToday\")\n",
        "selected1.show(5)"
      ],
      "metadata": {
        "id": "6krH_79czbPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_predictions1 = dtModel1.transform(testData)\n",
        "print('Decision Tree Accuracy (entropy):', multi_evaluator_ac.evaluate(dt_predictions1))\n",
        "print('Decision Tree F1 (entropy):', multi_evaluator_f1.evaluate(dt_predictions1))"
      ],
      "metadata": {
        "id": "pzcwkoUPzdaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}